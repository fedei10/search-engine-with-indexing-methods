{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4cQzHJa_bpx",
        "outputId": "6a9f3051-6618-45d2-8411-cd4e3d7d3ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --user nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter, defaultdict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqA611pAAVVU",
        "outputId": "c2b08d19-be10-4bfb-eeab-6beaa7c138e0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXo9_ejnAYFv",
        "outputId": "05b68fca-7b09-443a-82d6-48812c130518"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "Hit Enter to continue: punkt\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "Hit Enter to continue: snowball_data\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "Hit Enter to continue: q\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> x\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> x\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tp2"
      ],
      "metadata": {
        "id": "9Im3tBM_ACfy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting tokens"
      ],
      "metadata": {
        "id": "teNkj-VCBTVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"This is a test for text-based search\".split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UXdCdKlAEzQ",
        "outputId": "189f3a44-1f3f-49ed-d75f-f02cc9f51314"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'test', 'for', 'text-based', 'search']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \"This is a test for text-based search\".split()\n",
        "import re\n",
        "re.split(r\"\\W\", \"This is a test for text-based search\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dW3cv9eA0D4",
        "outputId": "dd488f0d-55bc-4600-850c-6d980c643822"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'test', 'for', 'text', 'based', 'search']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "case sensitivity"
      ],
      "metadata": {
        "id": "fqtdtVYNBQoH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X7l7Od1gEwKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \"Paris is the capital of France\".split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq0FNNkwA7bM",
        "outputId": "e0150111-2ff0-4041-d02a-55d2811bf11e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Paris', 'is', 'the', 'capital', 'of', 'France']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"Paris is the capital of France\".lower().split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCDL6fVNA9bY",
        "outputId": "93537566-da0b-4b9b-93ff-635871f62bb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paris', 'is', 'the', 'capital', 'of', 'france']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemming"
      ],
      "metadata": {
        "id": "SbxnNbXVBOn5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fud6tI3kEydy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "30mpCgFrBAgf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "LJQyyMqdBDn-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " [stemmer.stem(token) for token in [\"developed\", \"develop\", \"developing\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bneUAhL7BE71",
        "outputId": "2d9f29ef-cfc4-4459-e7b1-2408e71e9b5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['develop', 'develop', 'develop']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stopwords"
      ],
      "metadata": {
        "id": "VXtIBMz9BLLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = \"This is a test for text-based search\".lower().split()"
      ],
      "metadata": {
        "id": "ssMbvqNpBGj8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[t for t in tokens if t not in [\"a\", \"for\", \"is\", \"this\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUH8yHucBIw_",
        "outputId": "d0e9ff54-7d74-41d3-9263-57ecf2f1cb9d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'text-based', 'search']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "filtering"
      ],
      "metadata": {
        "id": "F21NfnS5BXpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import re"
      ],
      "metadata": {
        "id": "cWH5bmIIBaeO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = \"1234 test1234 rejected\".lower().split()"
      ],
      "metadata": {
        "id": "dOtZhj42BdNX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " [t for t in tokens if re.match(r\"^[a-z]+$\", t)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OArWQQC5Beex",
        "outputId": "ea482d87-62f9-4613-b94d-8c34fa4ccf5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rejected']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exercice 2"
      ],
      "metadata": {
        "id": "PEidycR0BhLa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "9Agc3s87B1Uo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stop_words=[\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\",\n",
        "            \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\",\n",
        "            \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"]"
      ],
      "metadata": {
        "id": "pNfFJEjbB29q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "f=open(\"fiche.txt\",\"r\")\n",
        "t=f.read().lower().split()\n",
        "new_t=[]\n",
        "for i in t:\n",
        "    if i not in stop_words and re.match(r\"^[a-z]+$\", i) :\n",
        "        new_t.append(stemmer.stem(i))\n",
        "print()\n",
        "print(new_t)\n",
        "f_sortie=open('sortie6.txt', 'w')\n",
        "for i in new_t:\n",
        "    print(i)\n",
        "    f_sortie.write(str(i)+\"\\n\")\n",
        "f_sortie.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C414HDliB46y",
        "outputId": "6ca7b94d-c563-482b-8adc-58ce7205057e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['bustl', 'citi', 'where', 'car', 'honk', 'peopl', 'lie', 'hidden', 'it', 'lush', 'greeneri', 'color', 'flower', 'provid', 'sanctuari', 'from', 'chao', 'bird', 'chirp', 'while', 'gentl', 'breez', 'whisper', 'secret', 'through', 'tranquil', 'time', 'seem', 'stand', 'allow', 'weari', 'soul', 'find', 'solac']\n",
            "bustl\n",
            "citi\n",
            "where\n",
            "car\n",
            "honk\n",
            "peopl\n",
            "lie\n",
            "hidden\n",
            "it\n",
            "lush\n",
            "greeneri\n",
            "color\n",
            "flower\n",
            "provid\n",
            "sanctuari\n",
            "from\n",
            "chao\n",
            "bird\n",
            "chirp\n",
            "while\n",
            "gentl\n",
            "breez\n",
            "whisper\n",
            "secret\n",
            "through\n",
            "tranquil\n",
            "time\n",
            "seem\n",
            "stand\n",
            "allow\n",
            "weari\n",
            "soul\n",
            "find\n",
            "solac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def open_files_in_folder(folder_path):\n",
        "\n",
        "    files = glob.glob(folder_path + '/*')\n",
        "\n",
        "    for file_path in files:\n",
        "        with open(file_path, 'r') as file:\n",
        "            file_content = file.read()\n",
        "            res=file_content.lower().split()\n",
        "            print(stm(file_path))\n",
        "folder_path = 'documents'\n",
        "\n",
        "open_files_in_folder(folder_path)"
      ],
      "metadata": {
        "id": "wgX9drnzCDcb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(doc):\n",
        "    from nltk.stem.porter import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(token) for token in doc]\n",
        "def tf_simple(doc):\n",
        "    d={}\n",
        "    for word in set(doc):\n",
        "        d[word]=doc.count(word)\n",
        "    return d\n",
        "\n",
        "def poids(dict_globale):\n",
        "    with open(\"fichier_inverse.txt\",\"w\")as f:\n",
        "        list_poids=[]\n",
        "        x=0\n",
        "        for dict in dict_globale:\n",
        "            x=x+1\n",
        "            poid_doc={}\n",
        "            for key,value in dict.items():\n",
        "                ni=0\n",
        "                for dict in dict_globale:\n",
        "                    if(key in list(dict)):\n",
        "                        ni=ni+1\n",
        "                poids=value/ni\n",
        "                f.write(f\"key{key}: doc_Num{x:} :poids-->{poids}\"+\"\\n\")\n",
        "                poid_doc[key]=poids\n",
        "            list_poids.append(poid_doc)\n",
        "        return list_poids\n",
        "\n",
        "dict_globale=[]\n",
        "tous_liste=[]\n",
        "\n",
        "for i in range(1,11):\n",
        "    with open(f\"doc{i}.txt\",'r') as f:\n",
        "        ch=f.read()\n",
        "        doc=ch.lower().split()\n",
        "        tous_liste.append(stem(doc))\n",
        "for doc in tous_liste:\n",
        "    dict_globale.append(tf_simple(doc))\n",
        "poids_tous_doc=poids(dict_globale)\n",
        "print(poids_tous_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnFKNGnxCGsn",
        "outputId": "827f4587-dff3-4288-edff-7e7706fdb59c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'musicien': 0.5, 'bien': 0.25, 'franco': 1.0, 'pour': 0.5, 'évoquez': 1.0, 'joli': 1.0, 'pa': 0.2, 'même': 1.0, 'trop': 0.5, 'plié': 1.0, 'finisterr': 1.0, 'vo': 5.0, 'beau': 1.0, 'pie': 0.3333333333333333, 'vill': 2.0, 'capital': 1.0, 'barcelon': 1.0, 'aujourd’hui': 1.0, 'incendiair': 1.0, 'naturalisé': 1.0, 'innoc': 1.0, 'allé': 1.0, 'dormez': 1.0, 'tunisien': 1.0, 'tiraillé': 1.0, 'pari': 1.0, 'd’aubervilli': 1.0, 'enfant': 1.0, 'grenel': 1.0, 'cordonni': 1.0, 'vendiez': 1.0, 'baignez': 1.0, 'cordou': 1.0, 'baléar': 1.0, 'indochinoi': 1.0, 'fêter': 1.0, 'marai': 1.0, 'du': 0.6666666666666666, 'apatrid': 1.0, 'souvenir': 0.5, 'aux': 2.0, 'à': 0.14285714285714285, 'sur': 0.25, 'fréju': 2.0, 'bord': 1.0, 'papier': 2.0, 'on': 0.5, 'avoir': 1.0, 'liberté': 1.0, 'que': 0.25, 'mourez.': 1.0, 'milieu': 1.0, 'dan': 0.8, 'tou': 0.4, 'débauché': 1.0, 'soleil': 0.5, 'retour': 1.0, 'embauché': 1.0, 'a': 0.4, 'franc': 1.0, 'homm': 1.0, 'oiseaux': 1.0, 'disciplinair': 1.0, 'sa': 0.25, 'quelqu': 0.5, 'vieill': 1.0, 'javel': 1.0, 'la': 1.7142857142857142, 'le': 0.6666666666666666, 'si': 1.5, 'boumian': 1.0, 'chaqu': 1.0, 'prise': 1.0, 'avec': 0.14285714285714285, 'bête': 1.0, 'navarr': 1.0, 'vou': 1.75, 'mort': 1.0, 'quatorz': 1.0, 'terrass': 1.0, 'fait': 0.5, 'coloni': 1.0, 'désoeuvré': 1.0, 'grandi': 1.0, 'visag': 0.3333333333333333, 'rizièr': 1.0, 'fer': 0.5, 'do': 1.0, 'polack': 1.0, 'grand': 0.3333333333333333, 'sénégal': 1.0, 'ne': 0.125, 'ordur': 1.0, 'doux': 1.0, 'en': 0.6, 'de': 4.75, 'cobay': 1.0, 'monnai': 1.0, 'manoeuvr': 1.0, 'soir': 0.3333333333333333, 'rue': 0.5, 'bout': 0.5, 'terr': 0.5, 'défendu': 1.0, 'parqué': 1.0, 'et': 0.9, 'd’une': 1.0, 'mer': 0.5, 'une': 0.2, 'locaux': 1.0, 'lointain': 1.0, 'd’itali': 1.0, 'labour': 1.0, 'kabyl': 1.0, 'chapel': 1.0, 'trouvé': 1.0, 'forêt': 1.0, 'tôt': 1.0, 'au': 0.6666666666666666, 'vite': 1.0, 'vôtre': 1.0, 'doré': 0.5, 'noir': 1.0, 'étranger': 0.5, 'ébouillanteur': 1.0, 'où': 1.0, 'bastil': 1.0, 'cadencé': 1.0, 'étrang': 0.5, 'mal': 1.0, 'rescapé': 1.0, 'vie': 0.5, 'adolesc': 1.0, 'peu': 1.0, 'vivez': 1.0, 'saint-ouen': 1.0, 'autr': 0.3333333333333333, 'port': 2.0, 'qui': 0.5, 'templ': 1.0, 'pay': 2.0, 'café': 1.0, 'couteaux': 2.0, 'bomb': 1.0, 'venez': 1.0, 'fil': 1.0, 'boît': 1.0, 'cigar': 1.0, 'dépatrié': 1.0, 'juillet': 1.0, 'd’or': 1.0, 'ou': 0.25, 'retourné': 1.0, 'brûleur': 1.0, 'écho': 1.0, 'quai': 1.0, 'pêcheur': 1.0, 'renvoyé': 1.0, 'rosier': 1.0, 'jongleur': 1.0, 'dragon': 1.0, 'villag': 1.0, 'déporté': 1.0, 'petit': 1.0, 'esclav': 2.0, 'autrefoi': 1.0, 'ête': 2.0, 'soutier': 1.0, 'expatrié': 1.0}, {'oiseau.': 1.0, 'l’encr': 1.0, 'pupitr': 2.0, 'pa': 0.2, 'vont': 1.0, 'tout': 0.16666666666666666, 'chanson': 0.5, 'voit': 0.5, 'deux': 9.0, 'son': 0.3333333333333333, 'vitr': 1.0, 'un': 0.8, 'tranquillement.': 1.0, 'fichent': 1.0, 'l’entend': 1.0, 'enfant': 1.0, 'l’oiseau-lyr': 2.0, 'fair': 0.5, 'musiqu': 1.0, 'à': 0.2857142857142857, 'crie': 1.0, 'trente-deux': 1.0, 'oiseau!': 1.0, 'class': 1.0, 'redevienn': 2.0, 'caché': 1.0, 'tour': 2.0, 'dan': 0.4, 'tou': 0.6, 'l’appel': 1.0, 'porte-plum': 1.0, 'également.': 1.0, 'seize…': 2.0, 'a': 0.4, 'mai': 0.4, 'dit': 0.5, 'sa': 0.25, 'la': 0.5714285714285714, 'moi': 0.5, 'le': 1.5555555555555556, 'redevi': 3.0, 's’en': 3.0, 'voilà': 0.5, 'écoutent': 1.0, 'avec': 0.42857142857142855, 'ni': 2.0, 'craie': 1.0, 'qu’est-c': 1.0, 'l’enfant': 7.0, 'chant': 1.0, 'aurez': 1.0, 'pitre!': 1.0, 'ne': 0.25, 's’écroulent': 1.0, 'de': 0.375, 'rien': 0.2, '?': 1.0, 'pass': 1.0, 'sabl': 1.0, 'il': 0.25, 'camp': 1.0, 'répétez!': 2.0, 'entend': 2.0, 'et': 3.2, 'une': 0.2, ':': 2.0, 'surtout': 1.0, 'arbr': 1.0, 'joue': 5.0, 'font': 6.0, 'quatre…': 1.0, 'qu’il': 0.3333333333333333, 'professeur': 0.5, 'eau': 1.0, 'fini': 1.0, 'autr': 0.3333333333333333, 'qui': 0.16666666666666666, 'ciel': 1.0, 'maîtr': 1.0, 'l’oiseau': 3.0, 'façon': 1.0, 'sauve-moi': 1.0, 'leur': 1.0, 'mur': 1.0, 'alor': 0.3333333333333333, 'quatr': 10.0, 'quand': 1.0, 'falais': 1.0, 'seiz': 5.0, 'lui…': 1.0, 'vont.': 1.0, 'descend': 1.0, 'vou': 0.25, 'huit': 11.0}, {'à': 0.14285714285714285, 'sur': 0.25, 'oui': 1.0, 'soudain': 0.5, 'couleur': 1.0, 'est': 0.2, 'qu’il': 0.3333333333333333, 'on': 0.25, 'date': 1.0, 'visag': 0.3333333333333333, 'questionn': 1.0, 'professeur': 0.5, 'mot': 0.5, 'crai': 1.0, 'chiffr': 1.0, 'tout': 0.3333333333333333, 'prodig': 1.0, 'tou': 0.2, 'phrase': 1.0, 'problèm': 1.0, 'aim': 1.0, 'maîtr': 0.5, 'de': 0.25, 'fou': 1.0, 'tableau': 1.0, 'il': 0.875, 'rire': 1.0, 'dessin': 1.0, 'bonheur.': 1.0, 'mai': 0.2, 'posé': 1.0, 'dit': 1.0, 'tête': 0.5, 'et': 0.6, 'la': 0.14285714285714285, 'huée': 1.0, 'coeur': 0.3333333333333333, 'le': 1.8888888888888888, 'non': 1.0, 'sou': 1.0, 'debout': 1.0, 'enfant': 0.3333333333333333, 'ce': 0.25, 'prend': 1.0, 'avec': 0.42857142857142855, 'pièg': 1.0, 'malgré': 1.0, 'sont': 0.5, 'nom': 0.5, 'effac': 1.0, 'malheur': 1.0, 'au': 0.16666666666666666, 'menac': 1.0, 'noir': 0.5, 'du': 0.5}, {'vie': 0.5, 'donnent': 1.0, 'pa': 0.4, 'que': 0.5, 'plu': 0.2, 'seul': 1.0, 'm’en': 0.5, 'ne': 0.25, 'droit.': 1.0, 'voit': 0.5, 'il': 0.125, 'me': 1.0, 'je': 0.4, 'regard': 4.0, 'ma': 0.25, 'cela': 0.3333333333333333, 'et': 0.2, 'ceux': 1.0, 'le': 0.1111111111111111, 'non': 0.5, 'ce': 0.25, 'j’aim': 4.0, 'vou': 0.75}, {'portait': 1.0, 'l’appelait': 1.0, 'bien': 0.25, 'sanglant': 1.0, 'pour': 0.75, 'est': 0.4, 'corridor': 1.0, 'pa': 0.4, 'plu': 0.6, 'quoi': 2.0, 'tout': 0.3333333333333333, 'jour': 0.5, 'est-el': 1.0, 'jeté': 0.5, 'mémoir': 4.0, 'valait': 1.0, 'gross': 2.0, 'comment': 1.0, 'son': 0.3333333333333333, 'fragil': 1.0, 'un': 1.2, 'grand-pèr': 1.0, 'pri': 1.0, 'cela': 0.3333333333333333, 'peur': 0.5, 'devant': 0.5, 'saurien': 2.0, 'a-t-el': 1.0, 'bon': 0.5, 'mond': 1.0, 'dedan': 1.0, 'éleveur': 1.0, 'mauv': 3.0, 'mi': 0.5, 'fair': 1.0, 'choix': 1.0, 'mainten': 2.0, 'du': 0.3333333333333333, 'souvenir': 1.5, 'musiqu': 0.5, 'à': 0.2857142857142857, 'tranquil': 1.0, 'sur': 0.25, 'oui': 0.5, 'voulait': 0.5, 'croi': 1.0, 'sauterel': 1.0, 'enfin': 1.0, 'haut': 1.0, 'souvien': 2.0, 'que': 0.75, 'amnésiqu': 1.0, 'sien': 1.0, 'vert': 1.0, 'dan': 0.4, 'futilité': 1.0, 'grise': 1.0, 'tou': 0.4, 'tomb': 1.0, 'judicieux': 1.0, 'paternel': 1.0, 'd’osier': 1.0, 'a': 1.2, 'pui': 1.0, 'mai': 0.2, 'mon': 2.0, 'boul': 3.0, 'je': 1.0, 'miett': 1.0, 'dit': 0.25, 'sa': 0.25, 'connai': 1.0, 'l’homm': 1.0, 'la': 1.5714285714285714, 'le': 0.7777777777777778, 'heur': 1.0, 'fond': 1.0, 'voilà': 0.5, 'avec': 0.42857142857142855, 'd’un': 0.5, 'fait': 0.5, 'or': 1.0, 'panier': 1.0, 'renvers': 1.0, 'peut-êtr': 1.5, 'puisqu': 1.0, 'sanglot': 1.0, 'loge': 1.0, 'mot': 2.0, 'devenu': 1.0, 'grand': 1.6666666666666667, 'ne': 0.375, 'vaurien': 1.0, 'femm': 1.0, 'en': 0.8, 'de': 2.0, 'hindou': 1.0, 'temp': 1.0, 'rien': 0.4, 'est-c': 2.0, 'soir': 0.3333333333333333, 'ba': 1.0, 'il': 0.5, 'bout': 0.5, 'me': 3.0, 'tard': 0.5, 'n’avait': 1.0, 'monsieur': 2.0, 'tête': 0.5, 'et': 1.2, 'aura-t-el': 1.0, 'une': 0.4, 'tort': 0.5, 'vacanc': 1.0, 'parti': 1.0, 'ell': 1.0, 'concierg': 1.0, 'au': 0.3333333333333333, 'lettr': 1.0, 'dégringolé': 1.0, 'fin': 1.0, 'l’histoir': 1.0, 'était': 1.0, 'vingt-quatr': 1.0, 'rentrant': 1.0, 'gravat': 1.0, 'assassiné': 1.0, 'hier': 1.0, 'autr': 0.3333333333333333, 'chose': 1.0, 'qui': 0.6666666666666666, 'ça': 2.0, 'bretel': 1.0, 'l’air': 2.0, 'interdit': 1.0, 'par': 2.0, 'dire': 0.5, 'rose': 1.0, 'roug': 1.0, 'roulé': 1.0, 'qu’el': 1.0, 'maintenant.': 1.0, 'place': 1.0, 'l’escali': 1.0, 'ou': 0.75, 'sai': 0.3333333333333333, 'ma': 0.25, 'tiroir': 1.0, 'c’est': 0.6666666666666666, 'bleue': 1.0, 'j’ai': 0.5, 'nom': 1.0, 'petit': 0.6666666666666666, 'étiquett': 1.0, 'pourquoi': 0.5, 'finira': 1.0, 'lui': 1.0}, {'ang': 1.0, 'étrang': 2.0, 'ail': 1.0, 'est': 0.4, 'âne': 1.0, 'plu': 0.2, 'qu’étrang': 1.0, 'ne': 0.125, 'l’ang': 3.0, 'chose': 0.5, 'l’âne': 3.0, 'en': 0.4, 'rien': 0.2, 'dire': 1.0, 'pie': 0.3333333333333333, 'étrâne': 2.0, 'il': 0.125, 'tapant': 1.0, 'dit': 1.5, '!': 1.0, 'quelqu': 0.5, 's’envole.': 1.0, 'cela': 0.3333333333333333, 'et': 0.1, 'c’est': 0.6666666666666666, 'le': 0.1111111111111111, 'vous-mêm': 1.0, 'si': 0.5, 'haussant': 1.0, 'être': 2.0, 'veut': 2.0, 'pourtant': 1.0, 'du': 0.16666666666666666, 'étranger': 0.5}, {'prospectu': 1.0, 'à': 0.14285714285714285, 'comm': 1.0, 'est': 0.4, 'plein': 2.0, 'plaisir': 1.0, 'l’eau': 1.0, 'heureus': 1.0, 'l’armé': 1.0, 'l’éboueur': 1.0, 'cett': 1.0, 'plantent': 1.0, 'vont': 0.5, 'tout': 0.3333333333333333, 'm’en': 0.5, 'dan': 0.6, 'nous.': 1.0, 'donné': 0.5, 'jeté': 0.5, 'qui': 0.16666666666666666, 'l’ai': 1.0, 'mécaniqu': 1.0, 'charmant': 1.0, 'de': 0.375, 'dirai': 1.0, 'sali': 1.0, 'rue': 1.5, 'il': 0.125, 'tard': 0.5, 'désir': 1.0, 'grâce': 1.0, 'un': 0.4, 'je': 1.2, 'passer': 1.0, 'leur': 0.5, 'offens': 1.0, 'couler': 1.0, 'alor': 0.3333333333333333, 'pardonnez-moi': 1.0, 'l’épée': 1.0, 'éboueur': 1.0, 'froissé': 1.0, 'ma': 0.75, 'là': 1.0, 'chinoi': 1.0, 'et': 0.5, 'la': 0.14285714285714285, 'le': 0.2222222222222222, 'coeur': 0.3333333333333333, 'cristal': 1.0, 'sera': 1.0, 'cont': 1.0, 'homme-sandwich': 1.0, 'votr': 1.0, 'effacé': 1.0, 'm’a': 0.3333333333333333, 'ruisseau': 1.0, 'avec': 0.2857142857142857, 'valet': 1.0, 'salut': 1.0, 'plaie': 1.0, 'excus': 1.0, 'au': 0.16666666666666666, 'd’ogress': 1.0, 'vou': 1.0, 'du': 0.5, 'salu': 2.0}, {'chanc': 1.0, 'perdu': 1.0, 'à': 0.2857142857142857, 's’appel': 1.0, 'détesté': 1.0, 'on': 0.5, 'pa': 0.4, 'grain': 1.0, 'même': 0.5, 'aurait': 1.0, 'bienvenu': 1.0, 'ne': 0.25, 'donné': 0.5, 'qui': 0.3333333333333333, 'en': 0.2, 'de': 0.125, 'rien': 0.2, 'appelé': 1.0, 'aimé': 1.0, 'nom-là': 1.0, 'il': 0.125, 'a': 0.2, 'méprisé': 1.0, 'mai': 0.2, 'je': 0.4, 'mauvais': 1.0, 'ou': 0.5, 'sai': 0.6666666666666666, 'et': 0.1, 'la': 0.14285714285714285, 'moi': 0.5, 'jamais.': 1.0, 'destiné': 1.0, 'bon': 0.5, 'j’ai': 0.5, 'y': 1.0, 'désiré': 1.0, 'ce': 0.25, 'pu': 1.0, 'm’a': 0.6666666666666666, 'eu': 1.0, 'pourquoi': 0.5, 'm’appel': 1.0}, {'cheval': 1.0, 'bien': 0.25, 'pour': 0.5, 'lune': 2.0, 'est': 0.2, 'l’attrap': 1.0, 'voiles.': 1.0, 'plu': 0.2, 'tout': 1.5, 'au-dessu': 1.0, 'tournant': 1.0, 'fuyait': 4.0, 'pie': 1.3333333333333333, 'un': 0.6, 'bateau': 2.0, 'salué': 1.0, 'peur': 0.5, 'devant': 0.5, 'troi': 1.0, 'mi': 0.5, 'rouler': 2.0, 'sont': 0.5, 'notr': 1.0, 'étoil': 2.0, 'voie': 2.0, 'derrièr': 1.0, 'du': 0.3333333333333333, 'arrêté': 1.0, 'chercher': 1.0, 'à': 1.4285714285714286, 'sur': 1.25, 'soudain': 0.5, 'voulait': 1.0, 'avanc': 1.0, 'on': 0.75, 'l’a': 1.0, 'cinq': 1.0, 'dan': 0.2, 'tou': 0.2, 'soleil': 0.5, 'saumon': 1.0, 'a': 0.8, 'reven': 1.0, 'naufrag': 1.0, 'pui': 0.5, 'manivel': 1.0, 'mai': 0.2, 'nou': 8.0, 'la': 2.2857142857142856, 'le': 0.7777777777777778, 'doigt': 1.0, 'fond': 0.5, 's’est': 2.0, 'avec': 0.14285714285714285, 'rencontré': 4.0, 'partant': 1.0, 'promenait': 1.0, 'd’un': 0.5, 'plongeant': 1.0, 'fer': 2.0, 'l’hiver': 2.0, 'remercié': 1.0, 'chemin': 4.0, 'grand': 0.3333333333333333, 'ne': 0.125, 'autour': 7.0, 'en': 0.6, 'de': 2.75, 'maison': 2.0, 'il': 0.125, 'terr': 3.0, 'abîmer': 1.0, 'écrasé': 1.0, 'voitur': 1.0, 'et': 1.2, 'mise': 1.0, 'mer': 2.5, 'une': 0.2, 'garde-barrièr': 1.0, 'traver': 1.0, 'tort': 0.5, 'sous-marin': 1.0, 'revenu': 1.0, 'au': 0.16666666666666666, 'doré': 0.5, 'printemp': 1.0, 'c’était': 0.5, 'sortant': 1.0, 'voil': 1.0, 'fumé': 1.0, 'coquillag': 1.0, 'se': 6.0, 'qui': 0.8333333333333334, 'beaux': 1.0, 'oursin': 1.0, 'l’école': 1.0, 'avon': 4.0, 'pousser': 2.0, 'île': 1.0, 'alor': 0.3333333333333333, 'mousquetair': 1.0, 'fleur': 1.0, 'ma': 0.25, 'japon': 1.0, 'parfumé': 1.0, 'petit': 0.3333333333333333, 'emmené': 1.0, 'main': 1.0, 'wagon': 1.0, 'lui': 0.5}, {'apparu': 1.0, 'harp': 1.0, 'c’était': 0.5, 'blessur': 1.0, 'arc': 1.0, 'bien': 0.25, 'peut-êtr': 1.5, 'visag': 0.3333333333333333, 'musicien': 0.5, 'qu’il': 0.6666666666666666, 'pour': 0.25, 'que': 0.5, 'trop': 1.0, 'm’est': 1.0, 'l’amour': 1.0, 'plu': 0.2, 'tout': 0.3333333333333333, 'jour': 0.5, 'aprè': 1.0, 'ne': 0.25, 'chanson': 0.5, 'blessé': 3.0, 'de': 0.25, 'rien': 0.2, 'soir': 0.3333333333333333, 'son': 0.3333333333333333, 'un': 0.8, 'je': 0.8, 'brûlant': 2.0, 'sa': 0.25, 'ou': 0.25, 'dangereux': 1.0, 'flèche': 1.0, 'tendr': 1.0, 'sai': 1.3333333333333333, 'et': 0.2, 'long': 1.0, 'c’est': 0.6666666666666666, 'le': 0.1111111111111111, 'une': 0.4, 'coeur': 0.3333333333333333, 'ce': 0.5, 'archer': 1.0, 'm’a': 0.6666666666666666, 'l’amour.': 1.0, 'avec': 0.5714285714285714, 'toujour': 1.0, 'au': 0.16666666666666666}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tp3"
      ],
      "metadata": {
        "id": "lpfONhCxCLYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path):\n",
        "    with open(path, 'r', encoding='latin-1') as f:\n",
        "        return f.read()"
      ],
      "metadata": {
        "id": "yddGgIHECMox"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_stem(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    words = word_tokenize(text)\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words"
      ],
      "metadata": {
        "id": "NHpHWzoNCbd7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_words_per_document(folder_path):\n",
        "    documents = os.listdir(folder_path)\n",
        "    stemmed_words_documents = {}\n",
        "\n",
        "    for doc in documents:\n",
        "        path = os.path.join(folder_path, doc)\n",
        "        text = read_file(path)\n",
        "        stemmed_words = tokenize_and_stem(text)\n",
        "        stemmed_words_documents[doc] = stemmed_words\n",
        "\n",
        "    return stemmed_words_documents"
      ],
      "metadata": {
        "id": "Ejyy2yvUCc_z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_idf(stemmed_words_per_document):\n",
        "    idf = {}\n",
        "\n",
        "    words_per_document = {}\n",
        "    for stemmed_words in stemmed_words_per_document.values():\n",
        "        for word in set(stemmed_words):\n",
        "            words_per_document[word] = words_per_document.get(word, 0) + 1\n",
        "\n",
        "    for word, num_documents_containing_word in words_per_document.items():\n",
        "        idf[word] = 1 / num_documents_containing_word\n",
        "\n",
        "    return idf"
      ],
      "metadata": {
        "id": "MYzb7HlQC0-U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tf_idf_weights(stemmed_words_per_document):\n",
        "    idf = calculate_idf(stemmed_words_per_document)\n",
        "    tf_idf_weights = {}\n",
        "\n",
        "    for document, stemmed_words in stemmed_words_per_document.items():\n",
        "        tf_idf_weights[document] = {}\n",
        "        for word in set(stemmed_words):\n",
        "            tf_idf_weights[document][word] = stemmed_words.count(word) * idf[word]\n",
        "\n",
        "    return tf_idf_weights"
      ],
      "metadata": {
        "id": "jgwvW19YC3pd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverse_file(stemmed_words_per_document, tf_idf_weights):\n",
        "    with open(\"inv-file.txt\", 'w') as inverse_file:\n",
        "        for word, document_weights in tf_idf_weights.items():\n",
        "            inverse_file.write(f\"{word}----\")\n",
        "            for document, weight in document_weights.items():\n",
        "                inverse_file.write(f\"{document}-----{weight} \")\n",
        "            inverse_file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "UEa_H1bZC5Se"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_user_query():\n",
        "    query = input(\"Enter your query: \")\n",
        "    return query"
      ],
      "metadata": {
        "id": "H_0HsIb7C6bs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_user_query(query):\n",
        "    stemmed_query = tokenize_and_stem(query)\n",
        "    indexed_query = Counter(stemmed_query)\n",
        "    return indexed_query"
      ],
      "metadata": {
        "id": "M1thmoWmC7VS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_similarity_score(indexed_query, tf_idf_corpus):\n",
        "    similarity_score = 0\n",
        "\n",
        "    for word, query_weight in indexed_query.items():\n",
        "        if word in tf_idf_corpus:\n",
        "            similarity_score += query_weight * tf_idf_corpus[word]\n",
        "\n",
        "    return similarity_score"
      ],
      "metadata": {
        "id": "oKFA3EMaC8p0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_relevant_documents(indexed_query, tf_idf_corpus):\n",
        "    document_scores = defaultdict(float)\n",
        "\n",
        "    for document, tf_idf_document in tf_idf_corpus.items():\n",
        "        similarity_score = calculate_similarity_score(indexed_query, tf_idf_document)\n",
        "        document_scores[document] = similarity_score\n",
        "\n",
        "    relevant_documents = sorted(document_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return relevant_documents"
      ],
      "metadata": {
        "id": "WjAdELINC9-n"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "kWM8OI8_DHid"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "print(\"Current directory:\", current_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNFDqbHNDVm2",
        "outputId": "9483ca00-d94a-43cb-d652-36a22ee6774c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents_folder = \"/content/docs\"\n",
        "stemmed_words_per_doc = stem_words_per_document(documents_folder)\n",
        "tf_idf_weights = calculate_tf_idf_weights(stemmed_words_per_doc)\n",
        "build_inverse_file(stemmed_words_per_doc, tf_idf_weights)\n",
        "\n",
        "user_query = read_user_query()\n",
        "indexed_query = index_user_query(user_query)\n",
        "relevant_documents = find_relevant_documents(indexed_query, tf_idf_weights)\n",
        "\n",
        "print(\"Relevant documents for the query:\")\n",
        "for document, score in relevant_documents:\n",
        "    if score > 0:\n",
        "        print(f\"Document Name: {document}, Similarity Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA99Fh_wDEaV",
        "outputId": "901b5bff-54ad-492a-81ba-1c4089f98fcb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: and\n",
            "Relevant documents for the query:\n"
          ]
        }
      ]
    }
  ]
}